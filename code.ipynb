{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds\n",
    "rnd.seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of question pairs:  404351\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"questions.csv\")\n",
    "N = len(data)\n",
    "print('Number of question pairs: ', N)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 300000 Test set: 10240\n"
     ]
    }
   ],
   "source": [
    "# Splitting into train and test\n",
    "N_train = 300000\n",
    "N_test = 10240\n",
    "data_train = data[:N_train]\n",
    "data_test = data[N_train:N_train + N_test]\n",
    "print(\"Train set:\", len(data_train), \"Test set:\", len(data_test))\n",
    "del (data)  # remove to free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting duplicate questions\n",
    "Select only the question pairs that are duplicate to train the model. <br>\n",
    "We need to build two sets of questions as input for the Siamese network, assuming that question $q1_i$ (question $i$ in the first set) is a duplicate of $q2_i$ (question $i$ in the second set), but all other questions in the second set are not duplicates of $q1_i$.  \n",
    "The test set uses the original pairs of questions and the status describing if the questions are duplicates.\n",
    "\n",
    "We will start by identifying the indexes in the training set which correspond to duplicate questions. For this we will define a boolean variable `td_index`, which has value `True` if the index corresponds to duplicate questions and `False` otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate questions:  111486\n",
      "Indexes of first ten duplicate questions: [5, 7, 11, 12, 13, 15, 16, 18, 20, 29]\n"
     ]
    }
   ],
   "source": [
    "td_index = data_train['is_duplicate'] == 1\n",
    "td_index = [i for i, x in enumerate(td_index) if x]\n",
    "print('Number of duplicate questions: ', len(td_index))\n",
    "print('Indexes of first ten duplicate questions:', td_index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why are so many Quora users posting questions that are readily answered on Google?\n",
      "Why do people ask Quora questions which can be answered easily by Google?\n",
      "is_duplicate:  1\n"
     ]
    }
   ],
   "source": [
    "print(data_train['question1'][18])\n",
    "print(data_train['question2'][18])\n",
    "print('is_duplicate: ', data_train['is_duplicate'][18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, keep only the rows in the original training set that correspond to the rows where `td_index` is `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_train = np.array(data_train['question1'][td_index])\n",
    "Q2_train = np.array(data_train['question2'][td_index])\n",
    "\n",
    "Q1_test = np.array(data_test['question1'])\n",
    "Q2_test = np.array(data_test['question2'])\n",
    "y_test  = np.array(data_test['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING QUESTIONS:\n",
      "\n",
      "Question 1:  Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
      "Question 2:  I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me? \n",
      "\n",
      "Question 1:  What would a Trump presidency mean for current international masterâ€™s students on an F1 visa?\n",
      "Question 2:  How will a Trump presidency affect the students presently in US or planning to study in US? \n",
      "\n",
      "TESTING QUESTIONS:\n",
      "\n",
      "Question 1:  How do I prepare for interviews for cse?\n",
      "Question 2:  What is the best way to prepare for cse? \n",
      "\n",
      "is_duplicate = 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing what the data looks like\n",
    "print('TRAINING QUESTIONS:\\n')\n",
    "print('Question 1: ', Q1_train[0])\n",
    "print('Question 2: ', Q2_train[0], '\\n')\n",
    "print('Question 1: ', Q1_train[5])\n",
    "print('Question 2: ', Q2_train[5], '\\n')\n",
    "\n",
    "print('TESTING QUESTIONS:\\n')\n",
    "print('Question 1: ', Q1_test[0])\n",
    "print('Question 2: ', Q2_test[0], '\\n')\n",
    "print('is_duplicate =', y_test[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate questions:  111486\n",
      "The length of the training set is:   89188\n",
      "The length of the validation set is:  22298\n"
     ]
    }
   ],
   "source": [
    "# Splitting training data into training and validation\n",
    "# Splitting the data\n",
    "cut_off = int(len(Q1_train) * 0.8)\n",
    "train_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]\n",
    "val_Q1, val_Q2 = Q1_train[cut_off:], Q2_train[cut_off:]\n",
    "print('Number of duplicate questions: ', len(Q1_train))\n",
    "print(\"The length of the training set is:  \", len(train_Q1))\n",
    "print(\"The length of the validation set is: \", len(val_Q1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the questions\n",
    "\n",
    "The next step is to learn how to encode each of the questions as a list of numbers (integers). We will encode each word of the selected duplicate pairs with an index. \n",
    "\n",
    "We will start by learning a word dictionary, or vocabulary, containing all the words in your training dataset, which you will use to encode each word of the selected duplicate pairs with an index. \n",
    "\n",
    "For this we will be using the [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer from Keras. which will take care of everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "text_vectorization = tf.keras.layers.TextVectorization(output_mode='int',split='whitespace', standardize='strip_punctuation')\n",
    "text_vectorization.adapt(np.concatenate((Q1_train,Q2_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 36224\n"
     ]
    }
   ],
   "source": [
    "# Getting the vocabulary size\n",
    "print(f'Vocabulary size: {text_vectorization.vocabulary_size()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first question in the train set:\n",
      "\n",
      "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? \n",
      "\n",
      "encoded version:\n",
      "tf.Tensor(\n",
      "[ 6984     6   178    10  8988  2442 35393   761    13  6636 28205    31\n",
      "    28   483    45    98], shape=(16,), dtype=int64) \n",
      "\n",
      "first question in the test set:\n",
      "\n",
      "How do I prepare for interviews for cse? \n",
      "\n",
      "encoded version:\n",
      "tf.Tensor([    4     8     6   160    17  2079    17 11775], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print('first question in the train set:\\n')\n",
    "print(Q1_train[0], '\\n') \n",
    "print('encoded version:')\n",
    "print(text_vectorization(Q1_train[0]),'\\n')\n",
    "\n",
    "print('first question in the test set:\\n')\n",
    "print(Q1_test[0], '\\n')\n",
    "print('encoded version:')\n",
    "print(text_vectorization(Q1_test[0]) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Siamese Model\n",
    "\n",
    "A Siamese network is a neural network which uses the same weights while working in tandem on two different input vectors to compute comparable output vectors\n",
    "\n",
    "We will get the question as input, get it vectorized and embedded, run it through an LSTM layer, normalize $v_1$ and $v_2$, and finally get the corresponding cosine similarity for each pair of questions. Because of the implementation of the loss function we will see in the next section, we are not going to have the cosine similarity as output of our Siamese network, but rather $v_1$ and $v_2$. We will add the cosine distance step once we reach the classification step. \n",
    "\n",
    "To train the model, we will use the triplet loss. This loss makes use of a baseline (anchor) input that is compared to a positive (truthy) input and a negative (falsy) input. The (cosine) distance from the baseline input to the positive input is minimized, and the distance from the baseline input to the negative  input is maximized. Mathematically, we are trying to maximize the following.\n",
    "\n",
    "$$\\mathcal{L}(A, P, N)=\\max \\left(\\|\\mathrm{f}(A)-\\mathrm{f}(P)\\|^{2}-\\|\\mathrm{f}(A)-\\mathrm{f}(N)\\|^{2}+\\alpha, 0\\right),$$\n",
    "\n",
    "where $A$ is the anchor input, for example $q1_1$, $P$ is the duplicate input, for example, $q2_1$, and $N$ is the negative input (the non duplicate question), for example $q2_2$.<br>\n",
    "$\\alpha$ is a margin; we can think about it as a safety net, or by how much we want to push the duplicates from the non duplicates. This is the essence of the triplet loss. However, as we will see in the next section, we will be using a pretty smart trick to improve our training, known as hard negative mining. \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a tensorflow Siamese model\n",
    "def Siamese(text_vectorizer, vocab_size=36224, d_feature=128):\n",
    "\n",
    "    branch = tf.keras.models.Sequential(name='sequential') \n",
    "    # Add the text_vectorizer layer. \n",
    "    branch.add(text_vectorizer)\n",
    "    # Add the Embedding layer.\n",
    "    branch.add(tf.keras.layers.Embedding(vocab_size,d_feature,name='embedding'))\n",
    "    # Add the LSTM layer, we want to the LSTM layer to return sequences, not just one value. \n",
    "    branch.add(tf.keras.layers.LSTM(d_feature,return_sequences=True,name='LSTM'))\n",
    "    # Add the GlobalAveragePooling1D layer. Remember to call it 'mean' using the parameter `name`\n",
    "    branch.add(tf.keras.layers.GlobalAveragePooling1D(name='mean'))\n",
    "    # Add the normalizing layer using the Lambda function.`\n",
    "    branch.add(tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x),name='out'))\n",
    "    \n",
    "    # Define both inputs. Be mindful of the data type and size\n",
    "    input1 = tf.keras.layers.Input((1,), dtype=tf.string, name='input_1')\n",
    "    input2 = tf.keras.layers.Input((1,), dtype=tf.string, name='input_2')\n",
    "    # Define the output of each branch of our Siamese network. Remember that both branches have the same coefficients, \n",
    "    # but they each receive different inputs.\n",
    "    branch1 = branch(input1)\n",
    "    branch2 = branch(input2)\n",
    "    # Define the Concatenate layer. You should concatenate columns, we can fix this using the `axis`parameter. \n",
    "    # This layer is applied over the outputs of each branch of the Siamese network\n",
    "    conc = tf.keras.layers.Concatenate(axis=1, name='conc_1_2')([branch1, branch2]) \n",
    "    \n",
    "    return tf.keras.models.Model(inputs=[input1, input2], outputs=conc, name=\"SiameseModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseModel\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 128)                  4768256   ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " conc_1_2 (Concatenate)      (None, 256)                  0         ['sequential[0][0]',          \n",
      "                                                                     'sequential[1][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4768256 (18.19 MB)\n",
      "Trainable params: 4768256 (18.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVe  (None, None)              0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 128)         4636672   \n",
      "                                                                 \n",
      " LSTM (LSTM)                 (None, None, 128)         131584    \n",
      "                                                                 \n",
      " mean (GlobalAveragePooling  (None, 128)               0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " out (Lambda)                (None, 128)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4768256 (18.19 MB)\n",
      "Trainable params: 4768256 (18.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check the model\n",
    "model = Siamese(text_vectorization, vocab_size=text_vectorization.vocabulary_size())\n",
    "model.build(input_shape=None)\n",
    "model.summary()\n",
    "model.get_layer(name='sequential').summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Negative Mining\n",
    "\n",
    "\n",
    "You will now implement the `TripletLoss` with hard negative mining.<br>\n",
    "As explained in the lecture, you will be using all the questions from each batch to compute this loss. Positive examples are questions $q1_i$, and $q2_i$, while all the other combinations $q1_i$, $q2_j$ ($i\\neq j$), are considered negative examples. The loss will be composed of two terms. One term utilizes the mean of all the non duplicates, the second utilizes the *closest negative*. Our loss expression is then:\n",
    " \n",
    "\\begin{align}\n",
    " \\mathcal{Loss_1(A,P,N)} &=\\max \\left( -cos(A,P)  + mean_{neg} +\\alpha, 0\\right) \\\\\n",
    " \\mathcal{Loss_2(A,P,N)} &=\\max \\left( -cos(A,P)  + closest_{neg} +\\alpha, 0\\right) \\\\\n",
    "\\mathcal{Loss(A,P,N)} &= mean(Loss_1 + Loss_2) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the custom loss function\n",
    "def TripletLossFn(v1, v2,  margin=0.25):\n",
    "\n",
    "    # use `tf.linalg.matmul` to take the dot product of the two batches. \n",
    "    # transpose the second argument using `transpose_b=True`\n",
    "    scores = tf.linalg.matmul(v2,v1,transpose_b=True)\n",
    "\n",
    "    # calculate new batch size and cast it as the same datatype as scores. \n",
    "    batch_size = tf.cast(tf.shape(v1)[0], scores.dtype) \n",
    "\n",
    "    # use `tf.linalg.diag_part` to grab the cosine similarity of all positive examples\n",
    "    positive = tf.linalg.diag_part(scores)\n",
    "    # subtract the diagonal from scores. We can do this by creating a diagonal matrix with the values \n",
    "    # of all positive examples using `tf.linalg.diag`\n",
    "    negative_zero_on_duplicate = scores-tf.linalg.diag(positive)\n",
    "    \n",
    "    # use `tf.math.reduce_sum` on `negative_zero_on_duplicate` for `axis=1` and divide it by `(batch_size - 1)`\n",
    "    mean_negative = tf.math.reduce_sum(negative_zero_on_duplicate,axis=1)/(batch_size-1)\n",
    "    \n",
    "    # create a composition of two masks: \n",
    "    #Â the first mask to extract the diagonal elements, \n",
    "    # the second mask to extract elements in the negative_zero_on_duplicate matrix that are larger than the elements in the diagonal \n",
    "    mask_exclude_positives = tf.cast((tf.eye(batch_size)==1)|(negative_zero_on_duplicate>tf.expand_dims(positive,1)),\n",
    "                                    scores.dtype)\n",
    "\n",
    "    # multiply `mask_exclude_positives` with 2.0 and subtract it out of `negative_zero_on_duplicate`\n",
    "    negative_without_positive = negative_zero_on_duplicate-2.0*mask_exclude_positives\n",
    "    # print(scores)\n",
    "    # print(negative_without_positive)\n",
    "    # take the row by row `max` of `negative_without_positive`. \n",
    "    # Hint: `tf.math.reduce_max(negative_without_positive, axis = None\n",
    "    closest_negative = tf.math.reduce_max(negative_without_positive, axis =1)\n",
    "    # print(closest_negative)\n",
    "    # compute `tf.maximum` among 0.0 and `A`\n",
    "    # A = subtract `positive` from `margin` and add `closest_negative` \n",
    "    triplet_loss1 = tf.maximum(margin-positive+closest_negative,0.0)\n",
    "\n",
    "    # compute `tf.maximum` among 0.0 and `B`\n",
    "    # B = subtract `positive` from `margin` and add `mean_negative` \n",
    "    triplet_loss2 = tf.maximum(margin-positive+mean_negative,0.0)\n",
    "    # add the two losses together and take the `tf.math.reduce_sum` of it\n",
    "    triplet_loss = tf.math.reduce_sum(triplet_loss1+triplet_loss2)\n",
    "    \n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss: 0.7035076825158911\n"
     ]
    }
   ],
   "source": [
    "# Checking the function\n",
    "v1 = np.array([[0.26726124, 0.53452248, 0.80178373],[0.5178918 , 0.57543534, 0.63297887]])\n",
    "v2 = np.array([[ 0.26726124,  0.53452248,  0.80178373],[-0.5178918 , -0.57543534, -0.63297887]])\n",
    "print(\"Triplet Loss:\", TripletLossFn(v1,v2).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recognize it as a loss function, keras needs it to have two inputs: true labels, and output labels. We will not be using the true labels, but we still need to pass some dummy variable with size `(batch_size,)` for TensorFlow to accept it as a valid loss.\n",
    "\n",
    "Additionally, the `out` parameter must coincide with the output of your Siamese network, which is the concatenation of the processing of each of the inputs, so we need to extract $v_1$ and $v_2$ from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TripletLoss(labels, out, margin=0.25):\n",
    "    _, embedding_size = out.shape # get embedding size\n",
    "    v1 = out[:,:int(embedding_size/2)] # Extract v1 from out\n",
    "    v2 = out[:,int(embedding_size/2):] # Extract v2 from out\n",
    "    return TripletLossFn(v1, v2, margin=margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Now it's time to finally train our model. As usual, we have to define the cost function and the optimizer. We also have to build the actual model we will be training. \n",
    "\n",
    "To pass the input questions for training and validation we will use the iterator produced by [`tensorflow.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n",
    "\n",
    "We will now write a function that takes in our model to train it. To train our model we have to decide how many times we want to iterate over the entire data set; each iteration is defined as an `epoch`. For each epoch, we have to go over all the data, using our `Dataset` iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(((train_Q1, train_Q2),tf.constant([1]*len(train_Q1))))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(((val_Q1, val_Q2),tf.constant([1]*len(val_Q1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(Siamese, TripletLoss, text_vectorizer, train_dataset, val_dataset, d_feature=128, lr=0.01, epochs=5):\n",
    "\n",
    "    # Instantiate the Siamese model\n",
    "    model = Siamese(text_vectorizer,\n",
    "                    vocab_size = text_vectorizer.vocabulary_size(), #set vocab_size accordingly to the size of your vocabulary\n",
    "                    d_feature = d_feature)\n",
    "    # Compile the model\n",
    "    model.compile(loss=TripletLoss,\n",
    "                  optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=lr))\n",
    "    # Train the model \n",
    "    model.fit(train_dataset,\n",
    "              epochs = epochs,\n",
    "              validation_data = val_dataset,\n",
    "             )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the `train_model` function. We will be using a batch size of 256. \n",
    "\n",
    "To create the data generators we will be using the method `batch` for `Dataset` object. We will also call the `shuffle` method, to shuffle the dataset on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 256\n",
    "train_generator = train_dataset.shuffle(len(train_Q1),\n",
    "                                        seed=7, \n",
    "                                        reshuffle_each_iteration=True).batch(batch_size=batch_size)\n",
    "val_generator = val_dataset.shuffle(len(val_Q1), \n",
    "                                   seed=7,\n",
    "                                   reshuffle_each_iteration=True).batch(batch_size=batch_size)\n",
    "# model = train_model(Siamese, TripletLoss,text_vectorization, \n",
    "#                                             train_generator, \n",
    "#                                             val_generator, \n",
    "#                                             epochs=epochs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "\n",
    "For this, we load a pretrained model for predictiona and compute the cosine loss between each pair of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseModel\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 128)                  4768256   ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " conc_1_2 (Concatenate)      (None, 256)                  0         ['sequential[0][0]',          \n",
      "                                                                     'sequential[1][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4768256 (18.19 MB)\n",
      "Trainable params: 4768256 (18.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('model/trained_model.keras', safe_mode=False, compile=False)\n",
    "\n",
    "# Show the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(test_Q1, test_Q2, y_test, threshold, model, batch_size=64, verbose=True):\n",
    "    y_pred = []\n",
    "    test_gen = tf.data.Dataset.from_tensor_slices(((test_Q1, test_Q2),None)).batch(batch_size=batch_size)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    pred = model.predict(test_gen)\n",
    "#     print(pred.shape)\n",
    "    _, n_feat = pred.shape\n",
    "    v1 = pred[:,:int(n_feat/2)]\n",
    "    v2 = pred[:,int(n_feat/2):]\n",
    "    \n",
    "    # Compute the cosine similarity. Using `tf.math.reduce_sum`. \n",
    "    # Don't forget to use the appropriate axis argument.\n",
    "#     d  = v1*v2\n",
    "    d=tf.math.reduce_sum(v1*v2,axis=1)\n",
    "\n",
    "    # Check if d>threshold to make predictions\n",
    "    y_pred = tf.cast(d>threshold, tf.int32)\n",
    "    \n",
    "    # take the average of correct predictions to get the accuracy\n",
    "    accuracy=tf.cast(y_test==y_pred,tf.int32)\n",
    "\n",
    "    accuracy = tf.reduce_sum(accuracy,0)/len(y_pred)\n",
    "\n",
    "    # compute the confusion matrix using `tf.math.confusion_matrix`\n",
    "    cm = tf.math.confusion_matrix(y_test,y_pred)\n",
    "    \n",
    "    \n",
    "    return accuracy, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 3s 105ms/step\n",
      "Accuracy 0.7259765625\n",
      "Confusion matrix:\n",
      "[[4876 1506]\n",
      " [1300 2558]]\n"
     ]
    }
   ],
   "source": [
    "accuracy, cm = classify(Q1_test,Q2_test, y_test, 0.7, model,  batch_size = 512) \n",
    "print(\"Accuracy\", accuracy.numpy())\n",
    "print(f\"Confusion matrix:\\n{cm.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with custom questions\n",
    "\n",
    "Here we will test the model with our own questions. We will write a function `predict` which takes two questions as input and returns `True` or `False` depending on whether the question pair is a duplicate or not.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(question1, question2, threshold, model, verbose=False):\n",
    "    generator = tf.data.Dataset.from_tensor_slices((([question1], [question2]),None)).batch(batch_size=1)\n",
    "\n",
    "    # Call the predict method of your model and save the output into v1v2\n",
    "    v1v2 = model.predict(generator)\n",
    "    # Extract v1 and v2 from the model output\n",
    "    _,n_feat=v1v2.shape\n",
    "    v1 = v1v2[:,:int(n_feat/2)]\n",
    "    v2 = v1v2[:,int(n_feat/2):]\n",
    "    # Take the dot product to compute cos similarity of each pair of entries, v1, v2\n",
    "    # Since v1 and v2 are both vectors, use the function tf.math.reduce_sum instead of tf.linalg.matmul\n",
    "    d = tf.math.reduce_sum(v1*v2,axis=1)\n",
    "    # Is d greater than the threshold?\n",
    "    res = d>threshold\n",
    "\n",
    "    if(verbose):\n",
    "        print(\"Q1  = \", question1, \"\\nQ2  = \", question2)\n",
    "        print(\"d   = \", d.numpy())\n",
    "        print(\"res = \", res.numpy())\n",
    "\n",
    "    return res.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "Q1  =  When will I see you? \n",
      "Q2  =  When can I see you again?\n",
      "d   =  [0.8422111]\n",
      "res =  [ True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1\n",
    "question1 = \"When will I see you?\"\n",
    "question2 = \"When can I see you again?\"\n",
    "# 1 means it is duplicated, 0 otherwise\n",
    "predict(question1 , question2, 0.7, model, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "Q1  =  Do you think it is monday? \n",
      "Q2  =  Is it monday?\n",
      "d   =  [0.44877848]\n",
      "res =  [False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2\n",
    "question1 = \"Do you think it is monday?\"\n",
    "question2 = \"Is it monday?\"\n",
    "# 1 means it is duplicated, 0 otherwise\n",
    "predict(question1 , question2, 0.7, model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python del env",
   "language": "python",
   "name": "delete_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
